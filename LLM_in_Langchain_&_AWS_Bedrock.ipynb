{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO6qbLnLWuAXcqR1c8eTX+V"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#     **Handle multiple LLM models in Langchain and AWS Bedrock seamlessly**    âœ… ðŸ”¥  "
      ],
      "metadata": {
        "id": "Sq14fHPSblDC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-requisites\n",
        "# To be able to advance through this tutorial, you will need:\n",
        "\n",
        "A functional AWS account where you have admin permission .<br>\n",
        "Have access to multiple LLM models on AWS Bedrock <br>\n",
        "An AWS user setup on you laptop <br>\n",
        "A functional Python environnement\n",
        "Initialize the Jupyter lab\n",
        "Letâ€™s create the Jupyter lab setup. We will use pipenv to create a virtualenv for this project.<br> Creating a virtualenv is very important as it will make sure this project will run without harming your other ones.\n",
        "\n",
        "Also please remember to always save your code using for example github.\n",
        "\n",
        "      `Install pipenv`\n",
        "Letâ€™s begin by installing pipenv which will handle virtualenvs and python packages for us ."
      ],
      "metadata": {
        "id": "2qG616VKZNyT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruas24H7X8UH"
      },
      "outputs": [],
      "source": [
        "!pip install pipenv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pipenv install langchain boto3"
      ],
      "metadata": {
        "id": "lUWnHhlLYMTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create the configurable multi LLM Langchain chat\n",
        "Here we will create, using Langchain, a configurable LLM interface that can handle multiple LLM models in AWS Bedrock."
      ],
      "metadata": {
        "id": "oDsFxN8kaIKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.chat_models import BedrockChat\n",
        "from langchain_core.runnables import ConfigurableField"
      ],
      "metadata": {
        "id": "VD2FtssjYfyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Hereâ€™s what is happening in this code:\n",
        "\n",
        "1. We import the BedrockChat Langchain component that allows to use the AWS Bedrock models.\n",
        "2. We also import ConfigurableField which will allow us to make this component configurable.."
      ],
      "metadata": {
        "id": "MGTMVsH1aP_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aws_region_name = \"us-west-2\"\n",
        "credentials_profile_name = \"default\"\n",
        "claude_3_sonnet = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
        "mistral_large = \"mistral.mistral-large-2402-v1:0\"\n",
        "mistral_large_bedrock_chat = BedrockChat(\n",
        "    model_id=mistral_large,\n",
        "    credentials_profile_name=credentials_profile_name,\n",
        "    region_name=aws_region_name,\n",
        ")\n",
        "_model_alternatives = {\n",
        "    \"mistral_large\": mistral_large_bedrock_chat\n",
        "}\n",
        "\n",
        "claude_3_sonnet = BedrockChat(\n",
        "    model_id=claude_3_sonnet,\n",
        "    credentials_profile_name=credentials_profile_name,\n",
        "    region_name=aws_region_name,\n",
        ")\n",
        "bedrock_llm = claude_3_sonnet.configurable_alternatives(\n",
        "    which=ConfigurableField(\n",
        "        id=\"model\", name=\"Model\", description=\"The model that will be used\"\n",
        "    ),\n",
        "    default_key=\"claude_3_sonnet\",\n",
        "    **_model_alternatives,\n",
        ")"
      ],
      "metadata": {
        "id": "3c_6OpwAYish"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *we create the configurable LLM interface:*\n",
        "\n",
        "1. We put the claude_3_sonnet as the default one.\n",
        "2. We added an alternative which is the mistral_large model.\n",
        "3. We created the key model which will allow us to change the model we use."
      ],
      "metadata": {
        "id": "DkZ6a4NZayo2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bedrock_llm.invoke(\"who are you ?\")\n",
        "\n",
        "bedrock_llm \\\n",
        "    .with_config(configurable={\"model\": \"mistral_large\"}) \\\n",
        "    .invoke(\"who are you ?\")"
      ],
      "metadata": {
        "id": "sVnG-4qDYw-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The BedrockChat component to use the Claude 3 Sonnet and Mistral Large models.**\n",
        "# The _model_alternatives that will allows use to create a configurable LLM interface."
      ],
      "metadata": {
        "id": "CJ5xhilNafli"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "_MISTRAL_PROMPT = PromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "<s>[INST] You are a conversational AI designed to answer in a friendly way to a question.\n",
        "You should always answer in rhymes.\n",
        "\n",
        "Human:\n",
        "<human_reply>\n",
        "{input}\n",
        "</human_reply>\n",
        "\n",
        "Generate the AI's response.[/INST]</s>\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "_CLAUDE_PROMPT = PromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "You are a conversational AI designed to answer in a friendly way to a question.\n",
        "You should always answer in jokes.\n",
        "\n",
        "Human:\n",
        "<human_reply>\n",
        "{input}\n",
        "</human_reply>\n",
        "\n",
        "Assistant:\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "_CHAT_PROMPT_ALTERNATIVES = {\"mistral_large\": _MISTRAL_PROMPT}"
      ],
      "metadata": {
        "id": "1qc_A68lY5JG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We import and create PromptTemplate for Mistral and Claude which take into <br> account specificity of each models.<br>  For example, the Mistral model wants s [INST] inside the prompt.\n",
        "We create two really different prompts to show how powerful this system is as you could have as many prompt alternative as you want and choose them at invoke time.\n",
        "We create and alternative prompt which is the Mistral large one"
      ],
      "metadata": {
        "id": "sUButRUtbFDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CONFIGURABLE_CHAT_PROMPT = _CLAUDE_PROMPT.configurable_alternatives(\n",
        "    which=ConfigurableField(\n",
        "        id=\"model\",\n",
        "        name=\"Model\",\n",
        "        description=\"The model that will be used\",\n",
        "    ),\n",
        "    default_key=\"claude_3_sonnet\",\n",
        "    **_CHAT_PROMPT_ALTERNATIVES\n",
        ")\n"
      ],
      "metadata": {
        "id": "YgcYwJtiY8GO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "chain = CONFIGURABLE_CHAT_PROMPT | bedrock_llm | StrOutputParser()\n",
        "\n",
        "chain.invoke(input=\"What is a large language model ?\")\n",
        "\n",
        "bedrock_llm \\\n",
        "    .with_config(configurable={\"model\": \"mistral_large\"}) \\\n",
        "    .invoke(\"What is a large language model ?\")"
      ],
      "metadata": {
        "id": "j9BJLq-5ZEpv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}